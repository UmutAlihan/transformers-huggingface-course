{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f878b6-71a4-40ea-9e35-deadef3e6aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample for a sequence classifier training\n",
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Same as before\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# this part is new, previously not existing using pipeline because we were only using pretrained motel weights\n",
    "\n",
    "# This is new\n",
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26f6ad-d044-45d4-847a-f26b8f6a59a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "710195fe-27ca-4bd2-a4fa-5daf45019b87",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'save_to_disk' from 'datasets' (/opt/conda/lib/python3.9/site-packages/datasets/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_56/3092321380.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_to_disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#os.environ[\"HF_HOME\"] =\"/home/jovyan/work/datasets\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'save_to_disk' from 'datasets' (/opt/conda/lib/python3.9/site-packages/datasets/__init__.py)"
     ]
    }
   ],
   "source": [
    "# datasets provided from datasets library\n",
    "# you can customize where these datasets are downloaded&cached by setting the HF_HOME environment variable.\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset, save_to_disk\n",
    "\n",
    "#os.environ[\"HF_HOME\"] =\"/home/jovyan/work/datasets\"\n",
    "data_dir = \"/home/jovyan/work/datasets\"\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\", data_dir=data_dir)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "992a4358-8453-4178-954b-1025a765fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets.save_to_disk(dataset_dict_path=\"./dataset-saved-glue-mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05082493-39ea-4f2f-9519-232fd3ea536f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Tuition at four-year private colleges averaged $ 19,710 this year , up 6 percent from 2002 .',\n",
       " 'sentence2': 'For the current academic year , tuition at public colleges averaged $ 4,694 , up almost $ 600 from the year before .',\n",
       " 'label': 1,\n",
       " 'idx': 100}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accessing each data by calling Dataset object by indexing \n",
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[87]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf38032a-57af-424c-9300-b9b3e755c6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can inspect the features of dataset \n",
    "# e.g.: to see corresponding label to given label integer\n",
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d608fbc-f8d4-4694-87cc-cee9ea27b68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint = \"bert-base-uncased\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# tokenize all the sentence pairs in dataset\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72f2fb66-1c88-428a-a153-dca24acc29dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer is able to parse 2 sequences together, which is required for our currnent task 'paraphrasing related training'\n",
    "# token_type_ids here defines which token belongs to which sentence\n",
    "# token_type_ids are only returned when the model will know what to do with them, \n",
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "064944bb-2e3c-4d0b-b03d-eba17c70f66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'sentence',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'second',\n",
       " 'one',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoding tokens using input_ids gives us correctly seperated sentences using special tokens\n",
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27322b80-e3bd-4544-b513-4b6c2990103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding, trunctuation options of tokenizer class are compatible \n",
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff8cb702-ccdf-4fec-9cb7-761a19657925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c221d68a4d34c829943fdcf9a456244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ef6c019ec14f5c92c32d2ec1aa2821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12404e66a0048e188026aaff4debe9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset.map method: allows us some extra flexibility\n",
    "# if we need more preprocessing done than just tokenization: \n",
    "# The map method works by applying a function on each element of the dataset\n",
    "\n",
    "# This function takes a dictionary (like the items of our dataset)\n",
    "# and returns a new dictionary with the keys input_ids, attention_mask, and token_type_ids. \n",
    "# This will allow us to use the option batched=True in our call to map\n",
    "\n",
    "# You can use multiprocessing when applying your preprocessing function with Dataset.map by passing along a num_proc argument\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40ce560e-d35a-4231-a1fd-77a2e446cc8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorWithPadding(tokenizer=PreTrainedTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), padding=True, max_length=None, pad_to_multiple_of=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dynamic padding\n",
    "# In PyTorch, the function that is responsible for putting together samples inside a batch is called a collate function.\n",
    "# padding is done inside batches to avoid too long padded inputs among the entire dataset\n",
    "    # to only apply it as necessary on each batch and avoid having over-long inputs with a lot of padding\n",
    "\n",
    "# DataCollatorWithPadding function takes a tokenizer when you instantiate it and will do everything you need\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5c146d9a-14e9-45b6-8ee6-27575091e423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "samples = {\n",
    "    k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]\n",
    "}\n",
    "\n",
    "[len(x) for x in samples[\"input_ids\"]]\n",
    "# different lenghts of input_ids indicates that 'padding' action is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d5bff6b-411d-4d4d-92d6-aec6a5a57ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': torch.Size([8, 67]),\n",
       " 'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dynamic padding means the samples in this batch should all be padded to a length of 67, the maximum length inside the batch.\n",
    "# Without dynamic padding, all of the samples would have to be padded to the maximum length in the whole dataset\n",
    "\n",
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b962d-1fde-4d91-a840-e15986dac45e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
